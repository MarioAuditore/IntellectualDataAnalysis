{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Трансформеры\nВ этом домашнем задании мы рассмотим использование трансформеров в библиотеке PyTorch. Рассмотрим задачу языкового моделирования. Попробуем генерировать текст нейронной сетью. ","metadata":{"id":"ST5CKSpvtg9A"}},{"cell_type":"markdown","source":"Ссылка на данные - https://drive.google.com/drive/folders/1x1A4ElliUGBPnHladGMwPxPuGxI8Vnpu?usp=sharing","metadata":{"id":"af8KwaYptg9a"}},{"cell_type":"code","source":"# хороший тон, импортировать все необходимые библиотеки в одной ячейке ;)\n\nimport torch\nfrom torch import nn\nimport math\nimport numpy as np\nimport time","metadata":{"id":"BcnHY80Btg9e","execution":{"iopub.status.busy":"2021-12-26T13:44:59.612285Z","iopub.execute_input":"2021-12-26T13:44:59.612610Z","iopub.status.idle":"2021-12-26T13:45:01.065206Z","shell.execute_reply.started":"2021-12-26T13:44:59.612523Z","shell.execute_reply":"2021-12-26T13:45:01.064385Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Что такое языковое моделирование? Это предсказание вероятности следующего токена (слова или буквы) на основе предыдущих токенов. Математически это можно описать так:\n\n$$P(x_i|x_1, x_2 , ... , x_{i-1})$$ \n\nПоследовательность $$ x_1, x_2, ... x_{i-1} $$ называют контекстом.","metadata":{"id":"sOd2-OBStg9j"}},{"cell_type":"markdown","source":"## Задание 0 (0 баллов, но сделать нужно)\nПроставьте знаки неравенств, исходя из вашего опыта:\n$$ P(раму | мама, мыла) * P(папу | мама, мыла) $$ \n$$ P(столу | дорога, ложка, к) * P(обеду | дорога, ложка, к) $$\n$$ P(Евпатий | меня, зовут) * P(Ваня | меня, зовут) $$\n$$ P(журналы | я, часто ,читаю) * P(комиксы | я, часто ,читаю) $$\nПопробуйте объяснить выбор для каждого из примеров.","metadata":{"id":"v8fNzeUrtg9q"}},{"cell_type":"markdown","source":"Ответ : \\\n$$ P(раму | мама, мыла) = P(папу | мама, мыла)$$ Ибо мой батя - рама и я пошел в него \\\nА вообще тут сокрее чаще рама, чем папа, т.е. >\n$$ P(столу | дорога, ложка, к) < P(обеду | дорога, ложка, к) $$ Вообще не слышал первое выражение\n$$ P(Евпатий | меня, зовут) < P(Ваня | меня, зовут) $$ Евпатий реже встречается\n$$ P(журналы | я, часто ,читаю) = P(комиксы | я, часто ,читаю) $$ Зависит, конечно, от собеседников, но в разных кругах разная вероятность комиксов или журналов, так что в общем виде эти слова равновероятны при заданном условии.\n","metadata":{"id":"fUGcbDRKtg9s"}},{"cell_type":"markdown","source":"Если для некоторых из примеров проставить знаки достаточно просто, то на некоторые сложно сказать, какой овтет верный. Мы принимаем решение для данного задания исходя их опыта использования русского языка. Мы много читали на русском и слушали огромное количество русской речи. Обучение языковых моделей происходит по схожему принципу. \n\nМы хотим показать модели столько текстов, сколько можем и надеемся, что она наберется достаточно опыта, чтобы расставлять такие знаки неравества максимально схоже с человеком.","metadata":{"id":"e6MgK-Aztg9v"}},{"cell_type":"markdown","source":"## Задание 1 (0.5 балла)\nМы будем обучать языковую модель для предсказания следущей буквы. Такие языковые модели применяются в распозновании речи, так как предоставляют дополнительную информацию акустической модели при выборе следующего символа. Для начала, откройте файл с данными, посмотрите, какие символы входят в тексты, сколько их. Уберите из текста все символы переноса на новую строку и табуляцию.","metadata":{"id":"pSNYPzJAtg9y"}},{"cell_type":"code","source":"!pip install gdown\n!gdown --id 1ijpKzdj4d1b0zYKot3_380OWtiI-2UJX","metadata":{"id":"7Szm4PlDvhQ5","outputId":"ac641ed4-b7d2-4d5c-fc57-2e12364fbbca","execution":{"iopub.status.busy":"2021-12-26T13:45:01.066995Z","iopub.execute_input":"2021-12-26T13:45:01.067256Z","iopub.status.idle":"2021-12-26T13:45:23.445370Z","shell.execute_reply.started":"2021-12-26T13:45:01.067219Z","shell.execute_reply":"2021-12-26T13:45:23.444615Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path = 'small_corp_for_test.txt'\nfile = open(path, 'r')\ndata = file.readlines()\nfile.close()\nlen(data)","metadata":{"scrolled":true,"id":"FUThe2wDtg93","outputId":"82fdaccd-672c-4618-c96e-ea90648232be","execution":{"iopub.status.busy":"2021-12-26T13:45:23.448404Z","iopub.execute_input":"2021-12-26T13:45:23.448758Z","iopub.status.idle":"2021-12-26T13:45:23.752507Z","shell.execute_reply.started":"2021-12-26T13:45:23.448722Z","shell.execute_reply":"2021-12-26T13:45:23.751821Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# YOUR CODE HERE\nimport re\n\n\ndata_processed = []\nfor line in data:\n\n  data_processed.append(re.sub('[\\n\\t]','',line))","metadata":{"id":"P6EG_ysStg-F","execution":{"iopub.status.busy":"2021-12-26T13:45:23.754484Z","iopub.execute_input":"2021-12-26T13:45:23.754816Z","iopub.status.idle":"2021-12-26T13:45:25.457401Z","shell.execute_reply.started":"2021-12-26T13:45:23.754765Z","shell.execute_reply":"2021-12-26T13:45:25.456672Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data_processed[100]","metadata":{"id":"fVQhgfMYTltu","outputId":"dd3773f6-1e3f-4085-cd9a-e4272155a2ef","execution":{"iopub.status.busy":"2021-12-26T13:45:25.458948Z","iopub.execute_input":"2021-12-26T13:45:25.459235Z","iopub.status.idle":"2021-12-26T13:45:25.464822Z","shell.execute_reply.started":"2021-12-26T13:45:25.459198Z","shell.execute_reply":"2021-12-26T13:45:25.464124Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\n\nchar_count = defaultdict(int)\n\nfor word in data_processed:\n  chars = list(word)\n  for c in chars:\n    char_count[c] += 1","metadata":{"id":"tsAkjpajxqol","execution":{"iopub.status.busy":"2021-12-26T13:45:25.466206Z","iopub.execute_input":"2021-12-26T13:45:25.466684Z","iopub.status.idle":"2021-12-26T13:45:31.847821Z","shell.execute_reply.started":"2021-12-26T13:45:25.466649Z","shell.execute_reply":"2021-12-26T13:45:31.847077Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"char_count","metadata":{"id":"5a_lMuknyhve","outputId":"46419a06-868f-4399-b68b-904ea6f3522b","execution":{"iopub.status.busy":"2021-12-26T13:45:31.849270Z","iopub.execute_input":"2021-12-26T13:45:31.849527Z","iopub.status.idle":"2021-12-26T13:45:31.856215Z","shell.execute_reply.started":"2021-12-26T13:45:31.849493Z","shell.execute_reply":"2021-12-26T13:45:31.855387Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Задание 2 (0.5 балла)\nДля обучения модели требуется сначала подготовить текст в подходящий для нейросети вид. Важно также отметить, что нужно добавить два токена start и end, которые отвечают за начало и конец текста. Используйте [ и ] для этой задачи. Также нам нужен токен pad, чтобы заполнять им текст до требуемой длинны для формирования батча.\n\nРеализуйте метод preprocess класса Preprocessor. Он должен принимать на вход текст и длинну текста, которую мы ожидаем получить на выходе. Текст должен быть переведен в нижний регистр, в конец текста добавляется требуемое число pad токенов, далее текст векторизуется (каждому символу ставится свое число). Вернуть требуется два вектора. Полученный результат без последнего токена (на нем будем обучаться) и полученный результат без первого токена (целевые метки при обучении).","metadata":{"id":"fyzP6Qtotg-L"}},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self):\n        self.alphabet = '_добсркгаупитнезчмфяжлйвцыэь-шхющёъ][ '\n        self.token2ind = {}\n        self.ind2token = {}\n        for i in range(len(self.alphabet)):\n            self.token2ind[self.alphabet[i]] = i\n            self.ind2token[i] = self.alphabet[i]\n        \n    \n    def preprocess(self, text, window_size):\n        # YOUR CODE HERE\n        vec_full = []\n        for c in text.lower():\n          vec_full.append(self.token2ind[c])\n\n        return vec_full + [0] * (window_size - len(vec_full)), vec_full[1:] + [0] * (window_size - len(vec_full[1:])) \n        #################","metadata":{"id":"4tr4GZv3tg-N","execution":{"iopub.status.busy":"2021-12-26T13:45:31.857761Z","iopub.execute_input":"2021-12-26T13:45:31.858065Z","iopub.status.idle":"2021-12-26T13:45:31.907171Z","shell.execute_reply.started":"2021-12-26T13:45:31.858030Z","shell.execute_reply":"2021-12-26T13:45:31.906463Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"check = Preprocessor()\ncheck.preprocess('[раз два]', 10)","metadata":{"id":"dju0X1GkQHCV","outputId":"fdb413a4-3fd7-49e1-c576-07c9b46bdf4b","execution":{"iopub.status.busy":"2021-12-26T13:45:31.908411Z","iopub.execute_input":"2021-12-26T13:45:31.909381Z","iopub.status.idle":"2021-12-26T13:45:31.917573Z","shell.execute_reply.started":"2021-12-26T13:45:31.909340Z","shell.execute_reply":"2021-12-26T13:45:31.916851Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Задание 3 (0.5 балла)\nТак как мы решили, что текст будет начинаться токеном [ и заканчиваться токеном ], данные нужно поправить. Реализуйте эту идею, добавьте данные токены в ваши тексты.","metadata":{"id":"s-nRcUNZtg-Q"}},{"cell_type":"code","source":"# YOUR CODE HERE\ndata_tokenised = ['[' + text + ']' for text in data_processed]\ndata_tokenised[100]\n################","metadata":{"id":"_3ieT9nrtg-R","outputId":"89f0b23f-7675-40fb-c2bf-dd0c9de5896c","execution":{"iopub.status.busy":"2021-12-26T13:45:31.921248Z","iopub.execute_input":"2021-12-26T13:45:31.921492Z","iopub.status.idle":"2021-12-26T13:45:32.152721Z","shell.execute_reply.started":"2021-12-26T13:45:31.921459Z","shell.execute_reply":"2021-12-26T13:45:32.151959Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Задание 4 (0.5 балла)\nТак как мы не располагаем большими мощностями, то давайте ограничим максимальную длинну текста. Вы можете менять этот порог и тем самым уменьшать кол-во текстов в вашей выборке и увеличивая тем самым скорость обучения. Начнем же мы с 128. \nВыберите порог и оставьте только те тексты, длина которых не превосходит данный порог.\n\nДалее разбейте тексты на train и test, перемешайте тексты при разбиении, размер тестовой выборки должен быть 15% от общего числа текстов. ","metadata":{"id":"tgxE5BDCtg-T"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nTHRESHOLD = 128\n\n# YOUR CODE HERE\ndata_threshold = [text for text in data_tokenised if len(text) <= 128]\ndata_train, data_test = train_test_split(data_threshold, test_size= int(0.15 * len(data_threshold)), random_state=123, shuffle = True)\n################","metadata":{"id":"YQWUoFPftg-U","execution":{"iopub.status.busy":"2021-12-26T13:45:32.156579Z","iopub.execute_input":"2021-12-26T13:45:32.158632Z","iopub.status.idle":"2021-12-26T13:45:33.409959Z","shell.execute_reply.started":"2021-12-26T13:45:32.158595Z","shell.execute_reply":"2021-12-26T13:45:33.409049Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Задание 5 (2 балла)\nНапишем датасет. На вход датасету передается набор текстов, объект класса Preprocessor и размер окна, который вы выбрали в прошлом задании.\nРеализуйте методы __len__ и __getitem__.","metadata":{"id":"Z_xhH31-tg-Z"}},{"cell_type":"code","source":"class TextDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, x, preproc, win_size = 128):\n        # YOUR CODE HERE\n        self.x = x\n        self.preproc = preproc\n        self.win_size = win_size\n        ################\n    \n    def __len__(self):\n        # YOUR CODE HERE\n        return(len(self.x))\n        ################\n    \n    def __getitem__(self, idx):\n        # YOUR CODE HERE\n        vec_1, vec_2 = self.preproc.preprocess(self.x[idx], self.win_size) \n        return torch.tensor(vec_1), torch.tensor(vec_2)\n        ################","metadata":{"id":"raq0cdpvtg-b","execution":{"iopub.status.busy":"2021-12-26T13:45:33.411943Z","iopub.execute_input":"2021-12-26T13:45:33.412477Z","iopub.status.idle":"2021-12-26T13:45:33.419330Z","shell.execute_reply.started":"2021-12-26T13:45:33.412427Z","shell.execute_reply":"2021-12-26T13:45:33.418546Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"preproc = Preprocessor()\ntrain_dataset = TextDataset(data_train, preproc)\ntest_dataset = TextDataset(data_test, preproc)","metadata":{"id":"jHEHh2w1tg-c","execution":{"iopub.status.busy":"2021-12-26T13:45:33.420783Z","iopub.execute_input":"2021-12-26T13:45:33.421099Z","iopub.status.idle":"2021-12-26T13:45:33.433712Z","shell.execute_reply.started":"2021-12-26T13:45:33.421063Z","shell.execute_reply":"2021-12-26T13:45:33.432827Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Задание 6 (2 балла)\nНапишем модель. Класс для реализации positional encoding реализован за вас, он нужен, чтобы модель могла после получения эмбедингов понимать, на каком месте какой токен находится.\n\nЗаполните пропуски в классе модели. Гипперпараметры модели вам предлагается подобрать самостоятельно. Рекомендуется использовать не более 6 слоев в трансформере. В декореде испоьлзуйте две линейных слоя с функцией активации ReLU между ними.\n\n## Задание 6_1 (0 баллов, но надо ответить!)\nПри обучении языковой модели на основе трансформеров мы используем маскирование символов (как мы это делаем - уже реализовано). Напишите, почему мы это делаем? Почему это так важно?\\\nОтвет:\\\nМы хотим, чтобы модель обучалась, а не подглядывала в ответы. Поэтому мы последовательно скармливаем модели наши данные через маску, которая последовательно зануляет т.н. \"future tokens\" , т.е. будущие слова последовательности (на самом деле там выше верхней диагонали ставятся отрицательные бесконечности, которые должны по идее превратиться в ноль софтмаксом).","metadata":{"id":"OptbgoSYtg-e"}},{"cell_type":"markdown","source":"https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ \\\nhttps://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0","metadata":{"id":"kCKPMzHe5pQa"}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)","metadata":{"id":"HGBtQFxPtg-f","execution":{"iopub.status.busy":"2021-12-26T13:45:33.434644Z","iopub.execute_input":"2021-12-26T13:45:33.434862Z","iopub.status.idle":"2021-12-26T13:45:33.486216Z","shell.execute_reply.started":"2021-12-26T13:45:33.434831Z","shell.execute_reply":"2021-12-26T13:45:33.485496Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Embeddings: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html","metadata":{"id":"DAqBNsYx6oDW"}},{"cell_type":"code","source":"class LanguageModel(nn.Module):\n    def __init__(self, vocab_size, d_model=128, nhead=4 , dim_fc_layer=256, num_encoder_layers=5):\n        super(LanguageModel, self).__init__()\n        self.emb = nn.Embedding(vocab_size, d_model)  # можем менять d_model\n        self.pe = PositionalEncoding(d_model)\n        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)  \n        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=num_encoder_layers)  # максимально число слоев 6\n        self.decoder = nn.Sequential(\n            nn.Linear(d_model, dim_fc_layer),\n            nn.ReLU(),\n            nn.Linear(dim_fc_layer, vocab_size)\n            \n        )\n    \n    def forward(self, x, src_mask):\n        x = self.pe(self.emb(x)) # emb, then pe     \n        x = x.transpose(1, 0)\n        x = self.transformer_encoder(x, mask=src_mask) # transformer encoder with mask\n        x = self.decoder(x)\n        return x.transpose(1, 0)\n    \n    def generate_square_subsequent_mask(self, sz):\n        # А вот и то самое маскирование\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask","metadata":{"id":"msEm7wlRtg-g","execution":{"iopub.status.busy":"2021-12-26T13:45:33.487658Z","iopub.execute_input":"2021-12-26T13:45:33.488533Z","iopub.status.idle":"2021-12-26T13:45:33.499774Z","shell.execute_reply.started":"2021-12-26T13:45:33.488495Z","shell.execute_reply":"2021-12-26T13:45:33.499029Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model_check = LanguageModel(len('_добсркгаупитнезчмфяжлйвцыэь-шхющёъ][ '))","metadata":{"id":"tn_Tl57itg-h","execution":{"iopub.status.busy":"2021-12-26T13:45:33.501147Z","iopub.execute_input":"2021-12-26T13:45:33.501475Z","iopub.status.idle":"2021-12-26T13:45:33.652230Z","shell.execute_reply.started":"2021-12-26T13:45:33.501422Z","shell.execute_reply":"2021-12-26T13:45:33.651581Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Задание 7 (2,5 балла)\nФинишная прямая. Давайте реализуем класс для обучения модели и ее валидации. Следуйте указаниям в коде и заполните недостающие фрагменты в коде.","metadata":{"id":"VOquQPNztg-i"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n\nclass Trainer:\n    \n    def __init__(self, model, train_dataset, test_dataset):\n        \n        self.model = model\n        \n        self.train_batch_size = 128\n        self.test_batch_size = 128\n        \n        self.train_dataloader = DataLoader(train_dataset, batch_size=self.train_batch_size)\n        self.test_dataloader = DataLoader(test_dataset, batch_size=self.test_batch_size)\n        self.train_dataloader_size = len(self.train_dataloader)  \n        self.test_dataloader_size = len(self.test_dataloader)  \n        \n        if torch.cuda.is_available():\n          self.device = 'cuda:0'\n        else:\n          self.device = 'cpu'\n        \n        self.criterion = nn.CrossEntropyLoss(ignore_index=0) # используйте CrossEntrophyLoss, передайте в качетсве параметра \n                             # ignore index индекс символа _, чтобы модель не штрафовалась за то\n                             # что идет после закрывающего токена\n        \n        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n        \n        self.steps_to_print = 250\n        \n    def train_one_epoch(self, epoch_number):\n        step = 0\n        counted_loss = 0\n        current_time = time.time()\n        it = 0  \n        \n        for batch in self.train_dataloader:\n            x, y = batch\n            # YOUR CODE HERE\n            \n            x = x.to(self.device)\n            y = y.to(self.device)\n            \n            src_mask = model.generate_square_subsequent_mask(x.size(1)).to(self.device)\n\n            y_pred = model(x, src_mask)\n            loss = self.criterion(y_pred.permute(0,2,1), y) \n\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            counted_loss += loss.detach().item()  \n            step += 1\n            it += 1\n            # реализуйте шаги обучения модели\n            # сохраняйте значение ошибки в переменную counted_loss\n            \n            ################\n            \n            \n            if step%self.steps_to_print == 0:\n                result = 'Train epoch '+str(epoch_number)+' | '\n                result += 'Step '+str(step)+'/'+str(self.train_dataloader_size)+' | '\n                result += 'Counted loss '+str(counted_loss)+' | '\n                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n                result += 'time '+str(time.time() - current_time) + ' | '\n                print(result)\n                current_time = time.time()\n                counted_loss = 0\n                it = 0\n    \n    def validate_one_epoch(self, epoch_number):\n        step = 0\n        counted_loss = 0\n        current_time = time.time()\n        it = 0  \n        for batch in self.test_dataloader:\n            x, y = batch\n            \n            # YOUR CODE HERE\n            \n            x = x.to(self.device)\n            y = y.to(self.device)\n\n            src_mask = model.generate_square_subsequent_mask(x.size(1)).to(self.device)\n\n            y_pred = model(x, src_mask)\n\n            loss = self.criterion(y_pred.permute(0,2,1), y)\n    \n            counted_loss = loss.item()\n            step += 1\n            it += 1\n            # реализуйте шаги для теста модели\n            # помните, что данный метод уже запускается из \n            # блока with torch.no_grad(), а потому \n            # повторно его использовать не нужно\n            \n            ################\n            \n            if step%(self.steps_to_print//2) == 0:\n                result = 'Validate epoch '+str(epoch_number)+' | '\n                result += 'Step '+str(step)+'/'+str(self.test_dataloader_size)+' | '\n                result += 'Counted loss '+str(counted_loss)+' | '\n                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n                result += 'time '+str(time.time() - current_time) + ' | '\n                print(result)\n                current_time = time.time()\n                counted_loss = 0\n                it = 0\n        \n    def train(self, number_of_epochs):\n        model.to(self.device)\n        for epoch in range(1, number_of_epochs+1):\n            model.train()\n            self.train_one_epoch(epoch)\n            with torch.no_grad():\n                model.eval()\n                self.validate_one_epoch(epoch)\n            print()","metadata":{"id":"STm0Kz8Xtg-j","execution":{"iopub.status.busy":"2021-12-26T13:45:33.653715Z","iopub.execute_input":"2021-12-26T13:45:33.654194Z","iopub.status.idle":"2021-12-26T13:45:33.717300Z","shell.execute_reply.started":"2021-12-26T13:45:33.654158Z","shell.execute_reply":"2021-12-26T13:45:33.716460Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Что такое ppl? Перплексия. Ее можно интерпретировать как меру \"удивленности\" модели нужному символу. Чем меньше данная величина, тем лучше, ведь это значит, что модель если и сделала неправильный выбор, то не сильно удивлена своей ошибке.\n\nПроведите несколько экспериментов, посмотрите, при каких гипперпараметрах значение перплексии минимально.","metadata":{"id":"3CVaDhdatg-k"}},{"cell_type":"markdown","source":"Касательно экспериментов такие выводы. Очень сильно влияет число слоев энкодера: они делают обучение тяжелее, но при этом улучшают качество, так что их наверное надо брать побольше. Также хорошо брать d_model > 64 и линейный слой нейросети > 256.","metadata":{}},{"cell_type":"markdown","source":"## Задание 8 (0.5 балла)\nЗапустите обучение на нескольких эпохах. Ориентируйтесь на ваши вычислительные мощности и время работы. Вы всегда можете посчитать, сколько секунд уходит на один батч.","metadata":{"id":"zYFsD6fktg-m"}},{"cell_type":"markdown","source":"Detailed guide to transformers \\\nhttps://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1","metadata":{"id":"Hgn_7aDIFHMY"}},{"cell_type":"code","source":"# YOUR CODE HERE\nmodel = LanguageModel(len('_добсркгаупитнезчмфяжлйвцыэь-шхющёъ][ '), d_model = 512, nhead = 8, dim_fc_layer = 1024, num_encoder_layers=6)\ntrainer = Trainer(model, train_dataset, test_dataset)\ntrainer.train(7)\n###############","metadata":{"id":"aopMeiHFcPjp","outputId":"20aaf733-5d32-4477-b26d-a92da6c3abee","execution":{"iopub.status.busy":"2021-12-26T13:45:33.718424Z","iopub.execute_input":"2021-12-26T13:45:33.718701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Задание 9 (1 балл)\nИтак, давайте попробуем погенерировать текст нашей сеткой. Закончите функцию по генерации текста. Попробуйте сгенерировать какой-нибудь текст. Помните, что если вы хотите генерировать текст с нуля, то вы должны передать в качестве текста только токен start.\nПрекратите генерировать текст, если модель выдала токен end или длинна текста больше 150.","metadata":{"id":"KrctCA6Etg-q"}},{"cell_type":"code","source":"def generate_text(text, model, device):\n    model.eval()\n    x = []\n    \n    for letter in text:\n        x.append(preproc.token2ind[letter])\n    x = torch.from_numpy(np.array(x)).unsqueeze(1).to(device)\n    src_mask = model.generate_square_subsequent_mask(x.size(1)).to(device)\n    pred = model(x, src_mask)\n    ind = int(torch.argmax(pred[-1]))\n    \n    while ind == 36:\n        src_mask = model.generate_square_subsequent_mask(x.size(1)).to(device)\n        pred = model(x, src_mask)\n        ind = int(torch.argmax(pred[-1]))\n\n    if ind > 38:\n        print(f'Ошибка. Индекс вне словаря. ind = {ind}')\n        print(f'pred {pred.size()}\\n{pred}')\n        print(pred[-1])\n        return\n\n    text += preproc.ind2token[ind]\n    if ind == 35 or len(text) > 150:\n        return text\n    else:\n        return generate_text(text, model, device)","metadata":{"id":"x_zAQXAStg-q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Попробуем нагенерить на обычной нетронутой модели","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n  device = 'cuda:0'\nelse:\n  device = 'cpu'\nwith torch.no_grad():\n    model_check.to(device)\n    print(generate_text('[', model_check, device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Теперь на обученной","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n  device = 'cuda:0'\nelse:\n  device = 'cpu'\nwith torch.no_grad():\n#     model.to(device)\n    print(generate_text('[', model, device))   # иад is pain","metadata":{"id":"pbFGShJuca3B","outputId":"71dd56b7-8ea1-41b5-9349-96f26d24728e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n  device = 'cuda:0'\nelse:\n  device = 'cpu'\nwith torch.no_grad():\n    print(generate_text('[иад это', model, device)) # исчерпывающе","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n  device = 'cuda:0'\nelse:\n  device = 'cpu'\nwith torch.no_grad():\n    print(generate_text('[ ну и как там', model, device)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Задание 10* (Задание - бонус, 5 баллов за реализацию при условии, что сделаны прошлые задания)\nДавайте вспомним, что такое transfer learning. Мы хотим использовать уже предобученные эмбединги для нашей сети, чтобы наша сеть обучалась быстрее. Давайте попробуем обучить новую модель на уровне слов, а не символов, но для упрощения задачи используем предобученный слой из библиотеки Natasha, а вернее, ее блок Navec.","metadata":{"id":"5GbwlSJctg-r"}},{"cell_type":"markdown","source":"[Изучите](https://github.com/natasha/navec) то, как вставить слой в вашу нейронную сеть.","metadata":{"id":"c86DB6BStg-s"}},{"cell_type":"markdown","source":"Теперь мы хотим, чтобы на вход модели подавались слова, модифицируйте ваш датасет. Возвращайте теперь номер слова в словаре navec.","metadata":{"id":"BzdMXC3Wtg-t"}},{"cell_type":"code","source":"class TextDataset_Navec(torch.utils.data.Dataset):\n    \n    def __init__(self, x, win_size = 128):\n        # YOUR CODE HERE\n        self.navec = ...\n        ################\n    \n    def __len__(self):\n        # YOUR CODE HERE\n        ################\n    \n    def __getitem__(self, idx):\n        # YOUR CODE HERE\n        ################","metadata":{"id":"VzrsrlWutg-t","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Немного модифицируем модель. Теперь нам не нужны слои с трансформером, так как весь механизм внимания уже заложен в ембедингах. Давайте попробуем просто пройтись линейной головой над эмбедингами. Выберите параметры самостоятельно.","metadata":{"id":"z1kAWrXRtg-u"}},{"cell_type":"code","source":"class LanguageModel(nn.Module):\n    def __init__(self):\n        super(LanguageModel, self).__init__()\n        self.emb_navec = ...\n        self.head = ...\n    \n    def forward(self, x):\n        x = ... # emb\n        x = ... # head\n        return x","metadata":{"id":"kRXJx31Ztg-v","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь дело за малым! Надо немного модифицировать класс обучения, так как мы не используем маскирование, после чего можно приступить к тесту!","metadata":{"id":"hUBFXqwOtg-w"}},{"cell_type":"code","source":"class Trainer:\n    \n    def __init__(self, model, train_dataset, test_dataset):\n        \n        self.model = model\n        \n        self.train_batch_size = 64\n        self.test_batch_size = 64\n        \n        self.train_dataloader = ...\n        self.test_dataloader = ...\n        self.train_dataloader_size = ...\n        self.test_dataloader_size = ...\n        \n        self.device = 'cuda:0'\n        self.criterion = ... \n        \n        self.optimizer = ...\n        \n        self.steps_to_print = 1000\n        \n    def train_one_epoch(self, epoch_number):\n        step = 0\n        counted_loss = 0\n        current_time = time.time()\n        it = 0\n        \n        for batch in self.train_dataloader:\n            x, y = batch\n            # YOUR CODE HERE\n            \n            # реализуйте шаги обучения модели\n            # сохраняйте значение ошибки в переменную counted_loss\n            \n            ################\n            \n            \n            if step%self.steps_to_print == 0:\n                result = 'Train epoch '+str(epoch_number)+' | '\n                result += 'Step '+str(step)+'/'+str(self.train_dataloader_size)+' | '\n                result += 'Counted loss '+str(counted_loss)+' | '\n                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n                result += 'time '+str(time.time() - current_time) + ' | '\n                print(result)\n                current_time = time.time()\n                counted_loss = 0\n                it = 0\n    \n    def validate_one_epoch(self, epoch_number):\n        step = 0\n        counted_loss = 0\n        current_time = time.time()\n        it = 0\n        for batch in self.test_dataloader:\n            x, y = batch\n            \n            # YOUR CODE HERE\n            \n            # реализуйте шаги для теста модели\n            # помните, что данный метод уже запускается из \n            # блока with torch.no_grad(), а потому \n            # повторно его использовать не нужно\n            \n            ################\n            \n            if step%(self.steps_to_print//2) == 0:\n                result = 'Validate epoch '+str(epoch_number)+' | '\n                result += 'Step '+str(step)+'/'+str(self.test_dataloader_size)+' | '\n                result += 'Counted loss '+str(counted_loss)+' | '\n                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n                result += 'time '+str(time.time() - current_time) + ' | '\n                print(result)\n                current_time = time.time()\n                counted_loss = 0\n                it = 0\n        \n    def train(self, number_of_epochs):\n        model.to(self.device)\n        for epoch in range(1, number_of_epochs+1):\n            model.train()\n            self.train_one_epoch(epoch)\n            with torch.no_grad():\n                model.eval()\n                self.validate_one_epoch(epoch)\n            print()","metadata":{"id":"hr4OvuCwtg-w","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Запустите обучение. ","metadata":{"id":"A7rKKMMftg-x"}},{"cell_type":"code","source":"# YOUR CODE HERE\n###############","metadata":{"id":"yhbLolIRtg-y","trusted":true},"execution_count":null,"outputs":[]}]}
